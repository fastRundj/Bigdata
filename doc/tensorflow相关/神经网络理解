前馈神经网络就是一层的节点只有前面一层作为输入，并输出到后面一层，自身之间、与其它层之间都没有联系，由于数据是一层层向前传播的，因此称为前馈网络。

卷积神经网络是根据人的视觉特性，认为视觉都是从局部到全局认知的，因此不全部采用全连接（一般只有1-2个全连接层，甚至最近的研究建议取消CNN的全连接层），而是采用一个滑动窗口只处理一个局部，这种操作像一个滤波器，这个操作称为卷积操作（不是信号处理那个卷积操作，当然卷积也可以），这种网络就称为卷积神经网络。

目前流行的大部分网络就是前馈网络和递归网络，这两种网络一般都是BP网络；深度网络一般采用卷积操作，因此也属于卷积神经网络。在出现深度学习之前的那些网络，基本都是全连接的，则不属于卷积网络的范围，但大部分是前馈网络和BP网络。

前馈网络的反向传播原理，与自己思考理解的一样，以下链接可证明：
https://www.cnblogs.com/charlotte77/p/5629865.html

反馈是用来求偏导数的，偏导数是用来作梯度下降的，梯度下降是为了求得代价函数的极小值，使得期望和输出之间的误差尽可能的减小。

梯度消失：求得的偏导数太小，乘以学习速率n后更小，相乘得到的值就是要移动的步伐！移动的步伐太小，甚至为0，不移动了，叫做梯度消失，即是移动的梯度消失了。
梯度爆炸:是求偏导的过程中，得
到许多大于1的值，连乘后得到很大的偏导数，乘以学习速率n后，也是很大，乘以速率n得到的值就是要移动的步伐！移动的步伐太大，不合理，叫做梯度爆炸！



多轮机器人对话，填槽，槽位参考：https://zhuanlan.zhihu.com/p/30069151
上面说到底都在讲一个槽组的填写，也即一种信息的获取，但多轮对话的目的是将初步用户意图转化为明确用户指令，这其中所需要的信息通常都不只有一种。


实际业务场景中，完整的多轮对话过程通常会以树的形式存在，每个节点存在一个或多个槽组，用于获取一种或多种信息，节点间的槽组为依赖关系，节点内的槽组为平级关系。
上文将多轮对话定义为一件事情的处理，槽组/槽定义为一种信息的获取，槽位定义为信息的一种获取方式。这里我倾向于将多轮对话树结构中的一个节点定义为处理事情的一个步骤。
一件事情的处理包含多个步骤，每个步骤中需要补全一种或多种信息，每种信息存在一种或多种获取方式。

填槽意义：
结合上文，我们需要了解到，填槽的意义有两个：作条件分支多轮对话、作信息补全用户意图。换言之，填槽不仅是补全用户意图的方式，而且前序槽位的填写还会起到指导后续信息补全走向的作用。

分词、词性标注和命名实体识别等序列标注任务：https://www.jianshu.com/p/db278366d6b0

分词：去匹配字符串、基于两个词的同现频率（当成是一个词，当成句子的分隔符），来切词。
词性标注：就是给出一个句子，输出一个词性序列。训练样本是一句句带有词性标注的句子。名词、动词、副词、形容词等等。
命名实体识别：给出一个句子，识别出其中的实体类别。训练样本是一句句进行过实体标注过的句子。不是实体标为NA。LOC,Person，ORG等等。


生成式、判定式模型？

CRF就是学到了一些词之间的限定特征：用LSTM，整体的预测accuracy是不错indeed, 但是会出现上述的错误：在B之后再来一个B。这个错误在CRF中是不存在的，因为CRF的特征函数的存在就是为了对given序列观察学习各种特征（n-gram，窗口），这些特征就是在限定窗口size下的各种词之间的关系。然后一般都会学到这样的一条规律（特征）：B后面接E，不会出现E。这个限定特征会使得CRF的预测结果不出现上述例子的错误。当然了，CRF还能学到更多的限定特征，那越多越好啊！

CRF是给定观察序列的条件下，计算整个标记序列的联合概率。而HMM是给定当前状态，计算下一个状态。


中文分词，简书说明：https://www.jianshu.com/p/e978053b0b95
分词对搜索引擎的重要影响？速度和准确性？分词用到了复方分词算法？很多的应用首先是基于分词的？
但如果分词速度太慢，即使准确性再高，对于搜索引擎来说也是不可用的，因为搜索引擎需要处理数以亿计的网页，如果分 词耗用的时间过长，会严重影响搜索引擎内容更新的速度。
分词问题：歧义识别、新词识别？
国外的一些东西有些不能拿来直接用，就是因为中文的分词没解决好，国外软件想要打入中国市场，也必须解决中文分词问题，相对中国人来说，更有优势？

中文分词对搜索引擎的作用：https://www.douban.com/note/587367359/

自然语言理解：
理解：关于机器是否能真正理解语言一直有争论。先抛开这个问题，我们看看人类对语言的理解是怎么样的。

聊天机器人技术实现流程：http://gitbook.cn/books/593d71ba4686067a2200aec6/index.html

一个开放话题场景下的生成式模型应该是最智能、符合我们预期的聊天机器人。

因此，尽管聊天系统都是针对文本理解的大方向，但目标不同决定了技术路线会有所偏重，但聊天功能是一个基础功能。


seq2seq的由来：
要知道，在以往的很多模型中，我们一般都说输入特征矩阵，每个样本对应矩阵中的某一行。就是说，无论是第一个样本还是最后一个样本，他们都有一样的特征维度。但是对于翻译这种例子，难道我们要让每一句话都有一样的字数吗，那样的话估计五言律诗和七言绝句又能大火一把了，哈哈。但是这不科学呀，
所以就有了 seq2seq 这种结构。

Seq2Seq技术，全称Sequence to Sequence，该技术突破了传统的固定大小输入问题框架，开通了将经典深度神经网络模型（DNNs）运用于翻译与智能问答这一类序列型（Sequence Based，项目间有固定的先后关系）任务的先河，并被证实在英语－法语翻译、英语－德语翻译以及人机短问快答的应用中有着不俗的表现。

Seq2Seq被提出于2014年，最早由两篇文章独立地阐述了它主要思想，分别是Google Brain团队的《Sequence to Sequence Learning with Neural Networks》和Yoshua Bengio团队的《Learning Phrase Representation using RNN Encoder-Decoder for Statistical Machine Translation》。这两篇文章针对机器翻译的问题不谋而合地提出了相似的解决思路，Seq2Seq由此产生。

word embedding和 Word2Vec工具：https://www.cnblogs.com/iloveai/p/word2vec.html
word embedding和 Word2Vec工具关系：2013年，Google开源了一款用于词向量计算的工具——word2vec，引起了工业界和学术界的关注。首先，
word2vec可以在百万数量级的词典和上亿的数据集上进行高效地训练；其次，该工具得到的训练结果——词向量（word embedding），可以很好地度量词与词之间的相似性。
随着深度学习（Deep Learning）在自然语言处理中应用的普及，很多人误以为word2vec是一种深度学习算法。其实word2vec算法的背后是一个浅层神经网络。另外需要强调的一点是
，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。
很多人以为word2vec指的是一个算法或模型，这也是一种谬误。接下来，本文将从统计语言模型出发，尽可能详细地介绍word2vec工具背后的算法模型的来龙去脉。

多轮对话概述：https://www.jianshu.com/p/cde686e81b15






